{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYnUGLApMGZi"
   },
   "source": [
    "Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5esHiuvvLrLO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8eWD9UiMJa5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JBmYfJ5rLyZu"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
    "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\n",
    "Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[3] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MF1LRUpMO9h"
   },
   "source": [
    "Install Spacy: spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsiQeRqZLycU",
    "outputId": "369a6f68-a11d-46b2-9ba1-2a8066197e0c"
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy\n",
    "\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph6IlxoHMUZ3"
   },
   "source": [
    "Import all library need to perform test synthesis\n",
    "\n",
    "\n",
    "*  spacy for Natural Language Processing.\n",
    "*  STOP_WORDS is a set of default stop words for English language model in SpaCy.\n",
    "* punctuation is a pre-initialized string which will give the all sets of punctuation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IWttCFiGLyfI"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9kQEPISMV0J"
   },
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVqRI7a-Lyje",
    "outputId": "1d86199e-ee7c-4ca5-f6dd-f92a68503969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cannot', 'done', 'wherein', 'three', 'because', 'do', 'have', 'whoever', '‘m', 'you', 'most', 'against', 'beside', 'as', 'beyond', \"'ll\", 'both', 'becoming', 'eight', 'somewhere', 'itself', 'regarding', 'everything', 'might', 'some', 'would', '‘re', 'hereafter', 'at', 'been', 'between', 'hereupon', 'if', 'of', 'sixty', 'us', 'herself', 'very', 'were', 'so', 'too', 'less', 'through', \"'d\", \"'s\", 'nobody', 'by', 'anything', 'five', 'formerly', 'did', 'beforehand', 'hers', 'will', 'yourself', 'out', 'else', 'down', 'each', 'in', 'since', 'least', 'back', 'which', '‘ll', 'neither', 'i', '’ll', 'indeed', 'empty', 'name', 'its', 'mostly', 'hundred', 'ten', 'therefore', 'seem', 'his', 'thence', 'used', 'further', 'sometimes', 'yours', 'upon', 'has', 'everyone', 'while', 'without', 'due', 'but', 'next', 'thereupon', 'third', 'otherwise', 'whence', 'whatever', 'something', 'get', 'who', 'quite', 'four', 'himself', 'that', 'with', 'always', 'first', 'last', 'across', 'nothing', 'after', 'thereby', 'up', 'top', 'amount', 'does', 'all', \"'m\", '’d', 'it', '’m', 'side', 'doing', 'seemed', 'n’t', 'per', 'again', 'twelve', 'under', 'many', 'for', 'make', 'these', 'whereupon', 'anyhow', 'say', 'various', 'mine', 'over', 'show', 'during', 'amongst', 'ourselves', 'nevertheless', 'though', 'what', 'forty', 'few', 'whose', 'perhaps', 'six', 'among', 'never', 'put', 'somehow', 'those', 'whom', 'enough', 'call', 'our', 'why', 'within', 'below', 'an', 'than', 'except', 'anyone', 'ours', 'once', 'hereby', 'latter', 'same', 'and', 'another', 'eleven', 'must', 'afterwards', 'she', 'thereafter', 'whereby', 'also', 'a', 'had', 'keep', 'much', 'own', 'part', 'someone', 'me', 'meanwhile', 'off', 'please', 'therein', 'besides', 'sometime', 'toward', 'n‘t', 'nowhere', 'made', 'being', 'should', 'they', 'where', 'noone', 'either', 'how', 'whereafter', 'several', 'yourselves', 'themselves', 'everywhere', 'no', '’re', 'there', 'can', 'thru', 'front', 'via', 'are', 'we', 'your', 'then', 'on', '‘ve', 'really', 'to', 'throughout', \"'ve\", 'anyway', 'any', 'others', 'into', 'well', \"'re\", 'such', 'yet', 'hence', 'fifteen', 'almost', 'full', 'just', 'serious', 'am', 'bottom', 'whenever', 'moreover', 'whereas', 'although', 'her', 'he', \"n't\", 'alone', 'became', 'fifty', 'about', 'give', 'latterly', 'even', 'namely', 'often', 'only', 'this', 'nor', 'was', '’s', 'take', 'thus', 'move', 'elsewhere', 'two', 'onto', 'him', 'their', 'ca', 'already', 'other', 'could', 're', 'before', 'wherever', 'unless', 'or', 'my', 'myself', 'nine', 'become', 'not', 'rather', '’ve', 'seems', 'former', 'may', 'every', 'around', 'anywhere', 'becomes', 'behind', 'more', 'go', 'when', 'from', 'here', 'none', 'towards', 'until', 'using', 'one', 'along', 'see', 'still', 'herein', 'twenty', 'whither', 'however', 'seeming', 'is', 'together', '‘d', 'above', '‘s', 'them', 'ever', 'now', 'whether', 'whole', 'be']\n"
     ]
    }
   ],
   "source": [
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTBhQjhgMXYn"
   },
   "source": [
    "spacy.load is used to load a model. spacy.load('en_core_web_sm') loads the model package en_core_web_sm. This will return a language object nlp containing all components and data needed to process text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A3gtpESDLynK"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBLh2_OXMYwI"
   },
   "source": [
    "Calling the nlp object on a string of text will return a processed Doc. During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WcUHkfboLyqS"
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOGYV8MNNzEB"
   },
   "source": [
    "Each Doc consists of individual tokens, and we can iterate over them. Now we will make a list of tokens called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3jELz4yLytH",
    "outputId": "364a3f4a-bca5-4d81-c283-8f828112da06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'There', 'are', 'broadly', 'two', 'types', 'of', 'extractive', 'summarization', 'tasks', 'depending', 'on', 'what', 'the', 'summarization', 'program', 'focuses', 'on', '.', 'The', 'first', 'is', 'generic', 'summarization', ',', 'which', 'focuses', 'on', 'obtaining', 'a', 'generic', 'summary', 'or', 'abstract', 'of', 'the', 'collection', '(', 'whether', 'documents', ',', 'or', 'sets', 'of', 'images', ',', 'or', 'videos', ',', 'news', 'stories', 'etc', '.', ')', '.', 'The', 'second', 'is', 'query', 'relevant', 'summarization', ',', 'sometimes', 'called', 'query', '-', 'based', 'summarization', ',', 'which', 'summarizes', 'objects', 'specific', 'to', 'a', 'query', '.', 'Summarization', 'systems', 'are', 'able', 'to', 'create', 'both', 'query', 'relevant', 'text', 'summaries', 'and', 'generic', 'machine', '-', 'generated', 'summaries', 'depending', 'on', 'what', 'the', 'user', 'needs', '.', '\\n', 'An', 'example', 'of', 'a', 'summarization', 'problem', 'is', 'document', 'summarization', ',', 'which', 'attempts', 'to', 'automatically', 'produce', 'an', 'abstract', 'from', 'a', 'given', 'document', '.', 'Sometimes', 'one', 'might', 'be', 'interested', 'in', 'generating', 'a', 'summary', 'from', 'a', 'single', 'source', 'document', ',', 'while', 'others', 'can', 'use', 'multiple', 'source', 'documents', '(', 'for', 'example', ',', 'a', 'cluster', 'of', 'articles', 'on', 'the', 'same', 'topic', ')', '.', 'This', 'problem', 'is', 'called', 'multi', '-', 'document', 'summarization', '.', 'A', 'related', 'application', 'is', 'summarizing', 'news', 'articles', '.', 'Imagine', 'a', 'system', ',', 'which', 'automatically', 'pulls', 'together', 'news', 'articles', 'on', 'a', 'given', 'topic', '(', 'from', 'the', 'web', ')', ',', 'and', 'concisely', 'represents', 'the', 'latest', 'news', 'as', 'a', 'summary', '.', '\\n', 'Image', 'collection', 'summarization', 'is', 'another', 'application', 'example', 'of', 'automatic', 'summarization', '.', 'It', 'consists', 'in', 'selecting', 'a', 'representative', 'set', 'of', 'images', 'from', 'a', 'larger', 'set', 'of', 'images.[3', ']', 'A', 'summary', 'in', 'this', 'context', 'is', 'useful', 'to', 'show', 'the', 'most', 'representative', 'images', 'of', 'results', 'in', 'an', 'image', 'collection', 'exploration', 'system', '.', 'Video', 'summarization', 'is', 'a', 'related', 'domain', ',', 'where', 'the', 'system', 'automatically', 'creates', 'a', 'trailer', 'of', 'a', 'long', 'video', '.', 'This', 'also', 'has', 'applications', 'in', 'consumer', 'or', 'personal', 'videos', ',', 'where', 'one', 'might', 'want', 'to', 'skip', 'the', 'boring', 'or', 'repetitive', 'actions', '.', 'Similarly', ',', 'in', 'surveillance', 'videos', ',', 'one', 'would', 'want', 'to', 'extract', 'important', 'and', 'suspicious', 'activity', ',', 'while', 'ignoring', 'all', 'the', 'boring', 'and', 'redundant', 'frames', 'captured', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGzT1ucaQNKu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSLzDAknN0AW"
   },
   "source": [
    "We can see that all the punctuation marks and special characters are included in the tokens. Now we will remove them. punctuation contains a string of all the punctuations but it does now conatin \\n. So we will add \\n in punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "vYBRpQ4-Lyvc",
    "outputId": "ee0f4b9d-65b4-442e-9482-fffaed154947"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = punctuation + '\\n'\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA4Lo5-oN1AQ"
   },
   "source": [
    "Now we will make the word frequency table. It will contain the number of occurrences of all the distinct words in the text which are not punctuations or stop words. We will create a dictionary named word_frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tlhIqvuLyyk",
    "outputId": "f2cd30a8-af6a-4a97-f9a8-6f980529b5a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broadly': 1, 'types': 1, 'extractive': 1, 'summarization': 11, 'tasks': 1, 'depending': 2, 'program': 1, 'focuses': 2, 'generic': 3, 'obtaining': 1, 'summary': 4, 'abstract': 2, 'collection': 3, 'documents': 2, 'sets': 1, 'images': 3, 'videos': 3, 'news': 4, 'stories': 1, 'etc': 1, 'second': 1, 'query': 4, 'relevant': 2, 'called': 2, 'based': 1, 'summarizes': 1, 'objects': 1, 'specific': 1, 'Summarization': 1, 'systems': 1, 'able': 1, 'create': 1, 'text': 1, 'summaries': 2, 'machine': 1, 'generated': 1, 'user': 1, 'needs': 1, 'example': 3, 'problem': 2, 'document': 4, 'attempts': 1, 'automatically': 3, 'produce': 1, 'given': 2, 'interested': 1, 'generating': 1, 'single': 1, 'source': 2, 'use': 1, 'multiple': 1, 'cluster': 1, 'articles': 3, 'topic': 2, 'multi': 1, 'related': 2, 'application': 2, 'summarizing': 1, 'Imagine': 1, 'system': 3, 'pulls': 1, 'web': 1, 'concisely': 1, 'represents': 1, 'latest': 1, 'Image': 1, 'automatic': 1, 'consists': 1, 'selecting': 1, 'representative': 2, 'set': 2, 'larger': 1, 'images.[3': 1, 'context': 1, 'useful': 1, 'results': 1, 'image': 1, 'exploration': 1, 'Video': 1, 'domain': 1, 'creates': 1, 'trailer': 1, 'long': 1, 'video': 1, 'applications': 1, 'consumer': 1, 'personal': 1, 'want': 2, 'skip': 1, 'boring': 2, 'repetitive': 1, 'actions': 1, 'Similarly': 1, 'surveillance': 1, 'extract': 1, 'important': 1, 'suspicious': 1, 'activity': 1, 'ignoring': 1, 'redundant': 1, 'frames': 1, 'captured': 1}\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = {}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "                \n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jtfwnjilbuwz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdiLvPpKN2P1"
   },
   "source": [
    "Now we will get the max_frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHgUDZMOLy1E",
    "outputId": "9a5c7f7c-3ec1-4990-a362-24cb0a47ce94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_frequency = max(word_frequencies.values())\n",
    "max_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuCM8OgaN3Qu"
   },
   "source": [
    "We will divide each frequency value in word_frequencies with the max_frequency to normalize the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BcyMmjIGLy45",
    "outputId": "cbd32980-6922-422b-f225-559f89040be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broadly': 0.09090909090909091, 'types': 0.09090909090909091, 'extractive': 0.09090909090909091, 'summarization': 1.0, 'tasks': 0.09090909090909091, 'depending': 0.18181818181818182, 'program': 0.09090909090909091, 'focuses': 0.18181818181818182, 'generic': 0.2727272727272727, 'obtaining': 0.09090909090909091, 'summary': 0.36363636363636365, 'abstract': 0.18181818181818182, 'collection': 0.2727272727272727, 'documents': 0.18181818181818182, 'sets': 0.09090909090909091, 'images': 0.2727272727272727, 'videos': 0.2727272727272727, 'news': 0.36363636363636365, 'stories': 0.09090909090909091, 'etc': 0.09090909090909091, 'second': 0.09090909090909091, 'query': 0.36363636363636365, 'relevant': 0.18181818181818182, 'called': 0.18181818181818182, 'based': 0.09090909090909091, 'summarizes': 0.09090909090909091, 'objects': 0.09090909090909091, 'specific': 0.09090909090909091, 'Summarization': 0.09090909090909091, 'systems': 0.09090909090909091, 'able': 0.09090909090909091, 'create': 0.09090909090909091, 'text': 0.09090909090909091, 'summaries': 0.18181818181818182, 'machine': 0.09090909090909091, 'generated': 0.09090909090909091, 'user': 0.09090909090909091, 'needs': 0.09090909090909091, 'example': 0.2727272727272727, 'problem': 0.18181818181818182, 'document': 0.36363636363636365, 'attempts': 0.09090909090909091, 'automatically': 0.2727272727272727, 'produce': 0.09090909090909091, 'given': 0.18181818181818182, 'interested': 0.09090909090909091, 'generating': 0.09090909090909091, 'single': 0.09090909090909091, 'source': 0.18181818181818182, 'use': 0.09090909090909091, 'multiple': 0.09090909090909091, 'cluster': 0.09090909090909091, 'articles': 0.2727272727272727, 'topic': 0.18181818181818182, 'multi': 0.09090909090909091, 'related': 0.18181818181818182, 'application': 0.18181818181818182, 'summarizing': 0.09090909090909091, 'Imagine': 0.09090909090909091, 'system': 0.2727272727272727, 'pulls': 0.09090909090909091, 'web': 0.09090909090909091, 'concisely': 0.09090909090909091, 'represents': 0.09090909090909091, 'latest': 0.09090909090909091, 'Image': 0.09090909090909091, 'automatic': 0.09090909090909091, 'consists': 0.09090909090909091, 'selecting': 0.09090909090909091, 'representative': 0.18181818181818182, 'set': 0.18181818181818182, 'larger': 0.09090909090909091, 'images.[3': 0.09090909090909091, 'context': 0.09090909090909091, 'useful': 0.09090909090909091, 'results': 0.09090909090909091, 'image': 0.09090909090909091, 'exploration': 0.09090909090909091, 'Video': 0.09090909090909091, 'domain': 0.09090909090909091, 'creates': 0.09090909090909091, 'trailer': 0.09090909090909091, 'long': 0.09090909090909091, 'video': 0.09090909090909091, 'applications': 0.09090909090909091, 'consumer': 0.09090909090909091, 'personal': 0.09090909090909091, 'want': 0.18181818181818182, 'skip': 0.09090909090909091, 'boring': 0.18181818181818182, 'repetitive': 0.09090909090909091, 'actions': 0.09090909090909091, 'Similarly': 0.09090909090909091, 'surveillance': 0.09090909090909091, 'extract': 0.09090909090909091, 'important': 0.09090909090909091, 'suspicious': 0.09090909090909091, 'activity': 0.09090909090909091, 'ignoring': 0.09090909090909091, 'redundant': 0.09090909090909091, 'frames': 0.09090909090909091, 'captured': 0.09090909090909091}\n"
     ]
    }
   ],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word]/max_frequency\n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IhUzvCUN4ZK"
   },
   "source": [
    "Now we will do sentence tokenization. The entire text is divided into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsp37ORKN5aY",
    "outputId": "a33c31e7-d651-4015-a2c1-687b7b5fdf81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on., The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.)., The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query., Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
      ", An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document., Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic)., This problem is called multi-document summarization., A related application is summarizing news articles., Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\n",
      ", Image collection summarization is another application example of automatic summarization., It consists in selecting a representative set of images from a larger set of images.[3], A summary in this context is useful to show the most representative images of results in an image collection exploration system., Video summarization is a related domain, where the system automatically creates a trailer of a long video., This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions., Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = [sent for sent in doc.sents]\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImlJkrrDN59n"
   },
   "source": [
    "Now we will calculate the sentence scores. The sentence score for a particular sentence is the sum of the normalized frequencies of the words in that sentence. All the sentences will be stored with their score in the dictionary sentence_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63eCHVdGN6bB",
    "outputId": "047e3b16-3991-4333-89c7-2c80f684a1e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       " There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.: 2.818181818181818,\n",
       " The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).: 3.9999999999999987,\n",
       " The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.: 3.909090909090909,\n",
       " Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.: 3.09090909090909,\n",
       " An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.: 3.9999999999999996,\n",
       " Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).: 2.545454545454545,\n",
       " This problem is called multi-document summarization.: 1.8181818181818183,\n",
       " A related application is summarizing news articles.: 1.0909090909090908,\n",
       " Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.: 2.727272727272727,\n",
       " Image collection summarization is another application example of automatic summarization.: 2.909090909090909,\n",
       " It consists in selecting a representative set of images from a larger set of images.[3]: 1.1818181818181817,\n",
       " A summary in this context is useful to show the most representative images of results in an image collection exploration system.: 1.818181818181818,\n",
       " Video summarization is a related domain, where the system automatically creates a trailer of a long video.: 2.2727272727272725,\n",
       " This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions.: 1.1818181818181817,\n",
       " Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.: 1.4545454545454544}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "                \n",
    "sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syoXAfjIN7AF"
   },
   "source": [
    "Now we are going to select 30% of the sentences having the largest scores. For this we are going to import nlargest from heapq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jUosE2vFN7j4"
   },
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbzLuJwuN8fI"
   },
   "source": [
    "We want the length of summary to be 30% of the original length which is 4. Hence the summary will have 4 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suHJtg04N9GM",
    "outputId": "7c3bc3f9-8a12-4dd1-d50c-1986232c6839"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_length = int(len(sentence_tokens)*0.3)\n",
    "select_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fasfk1W8N9n0"
   },
   "source": [
    "nlargest() will return a list with the select_length largest elements i.e. 4 largest elements from sentence_scores. key = sentence_scores.get specifies a function of one argument that is used to extract a comparison key from each element in sentence_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ckaf0lSyN-Q6",
    "outputId": "3e7513d5-5b13-40da-edc9-96e4c0b316be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.,\n",
       " The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).,\n",
       " The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.,\n",
       " Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMOvpt5LOvwM"
   },
   "source": [
    "Now we will combine this sentence together and make final string which contains the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7mRV011dOwWQ"
   },
   "outputs": [],
   "source": [
    "final_summary = [word.text for word in summary]\n",
    "summary = ' '.join(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrkAVyliOxH-"
   },
   "source": [
    "Now we will display the original text, the summary of the text and the lengths of the original text and the generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyjLDtxLOxur",
    "outputId": "20690e4e-3b13-4273-954c-6a14a5e50374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869\n",
      "\n",
      "There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
      "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\n",
      "Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[3] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGTHmwInOyzD",
    "outputId": "072068ce-5074-457c-d428-4179933a6be8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n",
      "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(summary))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmP3EJWKOzzM",
    "outputId": "827954ac-1ed3-4f57-ca3d-cd1ea41ae0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.09090909090909091\n",
      "0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_occurance = (max(word_frequencies.values()) + min(word_frequencies.values()))/2\n",
    "print(max(word_frequencies.values()))\n",
    "print(min(word_frequencies.values()))\n",
    "print(avg_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prSYCmK7O1I3",
    "outputId": "b0e6aea8-f5c2-4abe-9ada-04aa432a647c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarization\n"
     ]
    }
   ],
   "source": [
    "for x in word_frequencies:\n",
    "  if(word_frequencies[x] > .4):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clkE7v6qO1u6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pq3kj4riO2bD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "UhkHeIrhXFWP",
    "outputId": "a9e29f8b-0da5-4e19-82cf-a6ff02b2995d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThere are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\\nImage collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[3] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gj02qlOeXQ_8",
    "outputId": "21752ea4-fa19-47a8-c070-e4b77749d76b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_synthesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
